#  AWS re/Start ‚Äî Laborat√≥rio 3.4: Treinamento de um Modelo de Machine Learning

> **Registro t√©cnico de execu√ß√£o pr√°tica ‚Äî Treinamento oficial AWS re/Start**

---

##  Direitos Autorais

> **AWS:** O cen√°rio, arquitetura e scripts originais s√£o propriedade intelectual da **Amazon Web Services, Inc.** Esta documenta√ß√£o √© um registro de execu√ß√£o pr√°tica de treinamento oficial AWS.
>
> **Documenta√ß√£o:** Este relat√≥rio t√©cnico, an√°lises de sa√≠da e resolu√ß√£o dos desafios foram produzidos por **Kaylane Kimberly**.

---

##  √çndice

1. [Vis√£o Geral do Laborat√≥rio](#-vis√£o-geral-do-laborat√≥rio)
2. [Objetivos](#-objetivos)
3. [Pr√©-requisitos](#-pr√©-requisitos)
4. [Contexto T√©cnico ‚Äî Dataset & Algoritmo](#-contexto-t√©cnico--dataset--algoritmo)
5. [Acesso ao Ambiente AWS](#-acesso-ao-ambiente-aws)
6. [Tarefa 1 ‚Äî Acessar o Amazon SageMaker Notebook](#-tarefa-1--acessar-o-amazon-sagemaker-notebook)
7. [Tarefa 2 ‚Äî Abrir o Notebook do Laborat√≥rio](#-tarefa-2--abrir-o-notebook-do-laborat√≥rio)
8. [Conceitos Aplicados](#-conceitos-aplicados)
9. [Pipeline de Machine Learning](#-pipeline-de-machine-learning)
10. [Exemplo de C√≥digo ‚Äî Divis√£o e Treinamento](#-exemplo-de-c√≥digo--divis√£o-e-treinamento)
11. [Arquitetura do Ambiente](#-arquitetura-do-ambiente)
12. [Resultados Esperados](#-resultados-esperados)
13. [Conclus√£o](#-conclus√£o)
14. [Recursos de Refer√™ncia](#-recursos-de-refer√™ncia)

---

##  Vis√£o Geral do Laborat√≥rio

Este laborat√≥rio √© o **3.4** da trilha de Machine Learning do programa **AWS re/Start**. Ele d√° continuidade √† explora√ß√£o do **conjunto de dados de coluna vertebral biomec√¢nica** (*Biomechanical Vertebral Column Dataset*), avan√ßando para as etapas de prepara√ß√£o de dados e treinamento de modelo.

O laborat√≥rio √© executado dentro do **Amazon SageMaker**, utilizando o ambiente **JupyterLab** para escrever e executar c√≥digo Python. O algoritmo utilizado para treinar o modelo √© o **XGBoost** ‚Äî um dos algoritmos de boosting mais eficazes e amplamente utilizados em competi√ß√µes e projetos de ci√™ncia de dados.

| Informa√ß√£o | Detalhe |
|------------|---------|
| **N√∫mero do laborat√≥rio** | 3.4 |
| **Dura√ß√£o estimada** | 30 minutos |
| **Tempo ativo do ambiente** | 120 minutos |
| **Servi√ßo principal** | Amazon SageMaker |
| **Algoritmo** | XGBoost |
| **Linguagem** | Python 3 (kernel: `conda_python3`) |
| **Dataset** | Biomechanical Vertebral Column |

---

##  Objetivos

Ao concluir este laborat√≥rio, as seguintes habilidades pr√°ticas s√£o desenvolvidas:

-  Dividir um dataset em tr√™s subconjuntos: **treinamento**, **valida√ß√£o** e **teste**
-  Treinar um modelo de Machine Learning utilizando o algoritmo **XGBoost** no **Amazon SageMaker**

---

##  Pr√©-requisitos

Para execu√ß√£o do laborat√≥rio, os seguintes requisitos s√£o necess√°rios:

- Acesso a um computador com conex√£o Wi-Fi e sistema operacional **Windows**, **macOS** ou **Linux** (Ubuntu, SUSE ou Red Hat)
- Usu√°rios Windows: **acesso como administrador** ao computador
- Navegador de Internet atualizado: **Chrome**, **Firefox** ou **IE9+**
  >  Vers√µes anteriores ao IE9 n√£o s√£o compat√≠veis com o console AWS

---

##  Contexto T√©cnico ‚Äî Dataset & Algoritmo

### Dataset: Biomechanical Vertebral Column

O dataset utilizado cont√©m caracter√≠sticas biomec√¢nicas extra√≠das de pacientes com e sem problemas na coluna vertebral. Os atributos representam medi√ß√µes de √¢ngulos e dimens√µes da pelve e coluna, sendo utilizados para classificar pacientes como **normais** ou com alguma **patologia ortop√©dica**.

**Atributos t√≠picos do dataset:**

| Atributo | Descri√ß√£o |
|----------|-----------|
| `pelvic_incidence` | Incid√™ncia p√©lvica |
| `pelvic_tilt` | Inclina√ß√£o p√©lvica |
| `lumbar_lordosis_angle` | √Çngulo de lordose lombar |
| `sacral_slope` | Inclina√ß√£o sacral |
| `pelvic_radius` | Raio p√©lvico |
| `degree_spondylolisthesis` | Grau de espondilolistese |
| `class` | R√≥tulo ‚Äî Normal ou Patol√≥gico |

---

### Algoritmo: XGBoost

O **XGBoost** (*Extreme Gradient Boosting*) √© um algoritmo supervisionado baseado em **√°rvores de decis√£o com boosting por gradiente**. √â amplamente reconhecido por sua alta performance, efici√™ncia computacional e capacidade de lidar bem com dados tabulares.

**Principais caracter√≠sticas:**

- Combina m√∫ltiplas √°rvores de decis√£o fracas em um modelo forte (*ensemble*)
- Utiliza regulariza√ß√£o L1 e L2 para reduzir overfitting
- Suportado nativamente pelo **Amazon SageMaker** como algoritmo built-in
- Excelente desempenho em tarefas de classifica√ß√£o e regress√£o

---

##  Acesso ao Ambiente AWS

1. Clicar em **"Iniciar laborat√≥rio"** e aguardar o status `"Lab status: ready"`
2. Fechar o painel de status e clicar em **"AWS"** para abrir o console em nova aba
3. O login √© realizado **automaticamente** com as credenciais do ambiente sandbox

>  **Dica:** Caso o console n√£o abra em nova aba, verificar se o navegador est√° bloqueando pop-ups e permitir o acesso ao site.

---

##  Tarefa 1 ‚Äî Acessar o Amazon SageMaker Notebook

### Passo a passo:

**1. Navegar at√© o Amazon SageMaker:**

No Console de Gerenciamento da AWS, acessar o menu **"Servi√ßos"** e selecionar **Amazon SageMaker**.

**2. Localizar a inst√¢ncia de notebook:**

No painel de navega√ß√£o √† esquerda:
```
Bloco de anota√ß√µes ‚Üí Inst√¢ncias do bloco de anota√ß√µes
```

**3. Abrir o JupyterLab:**

Localizar a inst√¢ncia chamada **`MyNotebook`** e clicar em **"Open JupyterLab"** ao final da linha.

> O JupyterLab √© o ambiente de desenvolvimento interativo baseado em navegador fornecido pelo SageMaker para execu√ß√£o de notebooks Python.

---

##  Tarefa 2 ‚Äî Abrir o Notebook do Laborat√≥rio

### Passo a passo:

**1. Localizar o arquivo no JupyterLab:**

No painel esquerdo do JupyterLab (navegador de arquivos), localizar o arquivo:
```
pt_br/3_4-machinelearning.ipynb
```

**2. Abrir o notebook:**

Clicar duas vezes no arquivo `3_4-machinelearning.ipynb` para abri-lo.

**3. Selecionar o kernel (se solicitado):**

Caso uma janela de sele√ß√£o de kernel seja exibida:
```
Kernel: conda_python3  ‚Üí  Clique em "Selecionar"
```

**4. Executar o notebook:**

Seguir as instru√ß√µes e c√©lulas de c√≥digo presentes no notebook, executando cada c√©lula sequencialmente.

---

##  Conceitos Aplicados

### Divis√£o do Dataset em 3 Subconjuntos

A divis√£o do dataset √© uma pr√°tica fundamental em Machine Learning para garantir que o modelo seja treinado, ajustado e avaliado de forma justa e sem vazamento de dados (*data leakage*):

| Conjunto | Propor√ß√£o T√≠pica | Finalidade |
|----------|-----------------|------------|
| **Treinamento** | 70‚Äì80% | Alimentar o algoritmo para aprender os padr√µes dos dados |
| **Valida√ß√£o** | 10‚Äì15% | Ajustar hiperpar√¢metros e monitorar overfitting durante o treino |
| **Teste** | 10‚Äì15% | Avalia√ß√£o final do modelo ‚Äî dados nunca vistos durante o treino |

>  A separa√ß√£o correta dos conjuntos evita **overfitting** (modelo memoriza os dados de treino) e garante que a avalia√ß√£o de performance seja confi√°vel.

---

### Treinamento com XGBoost no SageMaker

O Amazon SageMaker disponibiliza o XGBoost como **algoritmo built-in**, o que permite treinar modelos de forma gerenciada sem necessidade de configurar servidores. O fluxo t√≠pico envolve:

1. Preparar e fazer upload dos dados para o **Amazon S3**
2. Configurar o **Estimator** do SageMaker com o container XGBoost
3. Definir os **hiperpar√¢metros** do modelo
4. Chamar o m√©todo `.fit()` para iniciar o treinamento
5. O SageMaker provisiona a infraestrutura, treina e encerra automaticamente

---

##  Exemplo de C√≥digo ‚Äî Divis√£o e Treinamento

Os trechos abaixo ilustram o fluxo t√≠pico executado no notebook do laborat√≥rio:

### 1. Importa√ß√µes e Carregamento dos Dados

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.inputs import TrainingInput
from sagemaker.image_uris import retrieve

# Carregar o dataset
df = pd.read_csv('vertebral_column_data.csv')
print(df.shape)
df.head()
```

### 2. Divis√£o em Treinamento, Valida√ß√£o e Teste

```python
# Separar features (X) e target (y)
X = df.drop(columns=['class'])
y = df['class']

# Primeira divis√£o: treino (70%) + tempor√°rio (30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

# Segunda divis√£o: valida√ß√£o (15%) + teste (15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

print(f"Treinamento : {X_train.shape[0]} amostras")
print(f"Valida√ß√£o   : {X_val.shape[0]} amostras")
print(f"Teste       : {X_test.shape[0]} amostras")
```

### 3. Preparar Dados para o SageMaker (formato CSV sem header)

```python
# O XGBoost no SageMaker exige que o target seja a primeira coluna
train_data = pd.concat([y_train.reset_index(drop=True),
                        X_train.reset_index(drop=True)], axis=1)
val_data   = pd.concat([y_val.reset_index(drop=True),
                        X_val.reset_index(drop=True)], axis=1)
test_data  = pd.concat([y_test.reset_index(drop=True),
                        X_test.reset_index(drop=True)], axis=1)

# Salvar localmente
train_data.to_csv('train.csv', index=False, header=False)
val_data.to_csv('validation.csv', index=False, header=False)
test_data.to_csv('test.csv', index=False, header=False)
```

### 4. Upload para o Amazon S3

```python
session    = sagemaker.Session()
role       = get_execution_role()
bucket     = session.default_bucket()
prefix     = 'lab34-xgboost'

# Upload dos datasets para o S3
s3_train = session.upload_data('train.csv',      bucket=bucket, key_prefix=f'{prefix}/train')
s3_val   = session.upload_data('validation.csv', bucket=bucket, key_prefix=f'{prefix}/validation')

print(f"Train URI : {s3_train}")
print(f"Val URI   : {s3_val}")
```

### 5. Configurar e Treinar o Modelo XGBoost

```python
# Obter a imagem do container XGBoost gerenciado pelo SageMaker
region         = boto3.Session().region_name
xgboost_image  = retrieve('xgboost', region=region, version='1.5-1')

# Configurar o Estimator
xgb_model = sagemaker.estimator.Estimator(
    image_uri       = xgboost_image,
    role            = role,
    instance_count  = 1,
    instance_type   = 'ml.m4.xlarge',
    output_path     = f's3://{bucket}/{prefix}/output',
    sagemaker_session = session
)

# Definir hiperpar√¢metros
xgb_model.set_hyperparameters(
    max_depth        = 5,
    eta              = 0.2,
    gamma            = 4,
    min_child_weight = 6,
    subsample        = 0.8,
    objective        = 'binary:logistic',
    num_round        = 100
)

# Configurar canais de entrada
train_input = TrainingInput(s3_train, content_type='text/csv')
val_input   = TrainingInput(s3_val,   content_type='text/csv')

# Iniciar treinamento
xgb_model.fit({'train': train_input, 'validation': val_input})
```

---

##  Arquitetura do Ambiente

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        AWS Account                           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ  Amazon SageMaker  ‚îÇ       ‚îÇ       Amazon S3           ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                    ‚îÇ       ‚îÇ                          ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ upload‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  JupyterLab  ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚îÇ train.csv          ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  (MyNotebook)‚îÇ  ‚îÇ       ‚îÇ  ‚îÇ validation.csv     ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ       ‚îÇ  ‚îÇ test.csv           ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ                    ‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  fit  ‚îÇ                          ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  XGBoost     ‚îÇ‚óÑ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  Training    ‚îÇ  ‚îÇoutput ‚îÇ  ‚îÇ  model artifacts   ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îÇ  Job         ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚îÇ  (model.tar.gz)    ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

##  Resultados Esperados

Ao executar o notebook com sucesso, os seguintes resultados s√£o obtidos:

**1. Divis√£o do dataset confirmada no output:**
```
Treinamento : 217 amostras
Valida√ß√£o   :  46 amostras
Teste       :  47 amostras
```

**2. Upload dos dados confirmado no S3:**
```
Train URI : s3://sagemaker-us-east-1-XXXX/lab34-xgboost/train/train.csv
Val URI   : s3://sagemaker-us-east-1-XXXX/lab34-xgboost/validation/validation.csv
```

**3. Logs do treinamento XGBoost durante o `.fit()`:**
```
[0] train-error:0.XX  validation-error:0.XX
[10] train-error:0.XX  validation-error:0.XX
...
[99] train-error:0.XX  validation-error:0.XX
```

**4. Status final do job de treinamento:**
```
Training job completed.
Billable seconds: XX
```

>  O laborat√≥rio √© considerado **conclu√≠do com √™xito** quando o job de treinamento do SageMaker finaliza sem erros e os artefatos do modelo s√£o salvos no S3.

---

## üèÅ Conclus√£o

Neste laborat√≥rio foram aplicadas as seguintes habilidades pr√°ticas:

-  **Divis√£o de dados** em tr√™s subconjuntos (treinamento, valida√ß√£o e teste) com estratifica√ß√£o para preservar a propor√ß√£o das classes
-  **Upload dos dados** para o Amazon S3, seguindo o formato esperado pelo algoritmo XGBoost do SageMaker
-  **Configura√ß√£o de um Estimator** no Amazon SageMaker com hiperpar√¢metros e canais de entrada definidos
-  **Treinamento gerenciado** de um modelo XGBoost, com o SageMaker provisionando e encerrando automaticamente a infraestrutura de computa√ß√£o
-  **Persist√™ncia dos artefatos** do modelo treinado no Amazon S3

Este laborat√≥rio consolida a compreens√£o do **ciclo de vida b√°sico de um modelo de Machine Learning** na AWS, desde a prepara√ß√£o dos dados at√© o artefato final pronto para ser implantado como endpoint de infer√™ncia.

---

##  Recursos de Refer√™ncia

| Recurso | Link |
|---------|------|
| Amazon SageMaker ‚Äî Documenta√ß√£o oficial | [docs.aws.amazon.com/sagemaker](https://docs.aws.amazon.com/sagemaker/) |
| XGBoost no SageMaker | [docs.aws.amazon.com/sagemaker/xgboost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) |
| Scikit-learn: train_test_split | [scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) |
| AWS Training & Certification | [aws.amazon.com/training](https://aws.amazon.com/training/) |
| UCI Vertebral Column Dataset | [archive.ics.uci.edu](https://archive.ics.uci.edu/ml/datasets/vertebral+column) |

---

<div align="center">

**¬© 2022 Amazon Web Services, Inc. e suas afiliadas. Todos os direitos reservados.**

*Este trabalho n√£o pode ser reproduzido nem redistribu√≠do, parcial ou integralmente, sem a permiss√£o pr√©via por escrito da Amazon Web Services, Inc. A c√≥pia, a venda e o empr√©stimo para fins comerciais s√£o proibidos.*

---

*Documenta√ß√£o t√©cnica produzida por **Kaylane Kimberly** como registro de execu√ß√£o pr√°tica do treinamento oficial AWS re/Start.*

</div>
